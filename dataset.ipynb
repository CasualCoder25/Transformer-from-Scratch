{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds):\n",
    "    for i in range(len(ds)):\n",
    "        yield ds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_or_get_tokenizer(ds,path):\n",
    "\n",
    "    tokenizer_path=path\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer=Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"])\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds),trainer=trainer)\n",
    "        tokenizer.save(tokenizer_path)\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(ds,path):\n",
    "    tokenizer_path=path\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        normalizer=normalizers.Sequence([NFD(), StripAccents()])\n",
    "        tokenizer.normalizer = normalizer\n",
    "        tokenizer.pre_tokenizer=Whitespace()\n",
    "        trainer = BpeTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"],vocab_size=30000)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds),trainer=trainer)\n",
    "        tokenizer.save(tokenizer_path)\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    return tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(tokenizer,input,output,input_size,output_size):\n",
    "\n",
    "    # Create input and output lists\n",
    "    input_list=[]\n",
    "    output_list=[]\n",
    "    # Filter through the data and choose suitable examples\n",
    "    for i in range(len(input)):\n",
    "        in_tokenized = tokenizer.encode(\"[SOS] \"+input[i]+\" [EOS]\").ids\n",
    "        out_tokenized = tokenizer.encode(output[i]+\" [EOS]\").ids\n",
    "        if len(in_tokenized)<=input_size and len(out_tokenized)<=output_size:\n",
    "            input_list.append(in_tokenized)\n",
    "            output_list.append(out_tokenized)\n",
    "    \n",
    "    return input_list,output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mapper(x,y,sos,input_size,output_size):\n",
    "\n",
    "    # Create encoder input\n",
    "    padding_length=input_size-len(x)\n",
    "    padding_length=max(0,padding_length)\n",
    "    encoder_input = tf.pad(x,np.array([[0,padding_length]]),constant_values=1)\n",
    "\n",
    "    # Create encoder mask\n",
    "    encoder_mask = np.arange(input_size)\n",
    "    encoder_mask = encoder_mask<len(x)\n",
    "    encoder_mask = encoder_mask.astype(int)\n",
    "    encoder_mask = encoder_mask.reshape(1,1,input_size)\n",
    "\n",
    "    # Create decoder input\n",
    "    padding_length=output_size-len(sos)\n",
    "    padding_length=max(0,padding_length)\n",
    "    decoder_input = tf.pad(sos,np.array([[0,padding_length]]),constant_values=1)\n",
    "    \n",
    "    # identify padding length for y\n",
    "    padding_length=output_size-len(y)\n",
    "    padding_length=max(0,padding_length)\n",
    "    decoder_output = tf.pad(y,np.array([[0,padding_length]]),constant_values=1)\n",
    "\n",
    "    # Create Decoder mask\n",
    "    decoder_mask = np.arange(output_size)\n",
    "    decoder_mask = decoder_mask<len(sos)\n",
    "    decoder_mask = decoder_mask.astype(int)\n",
    "    decoder_mask = decoder_mask.reshape(1,1,output_size)\n",
    "\n",
    "    return ((encoder_input,encoder_mask,decoder_input,decoder_mask),decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(data_path,tokenizer_path,input_size,output_size,split='train',key='INSTRUCTION',value='RESPONSE',get_tokenization=get_tokenizer):\n",
    "    ds_raw=load_dataset(data_path)\n",
    "    input=ds_raw[split][key]\n",
    "    output=ds_raw[split][value]\n",
    "    tokenizer_data=[input+output]\n",
    "\n",
    "    # Build Tokenizer\n",
    "    tokenizer = get_tokenization(tokenizer_data,tokenizer_path)\n",
    "\n",
    "    # Filter Data\n",
    "    input, output = filter_data(tokenizer,input, output,input_size,output_size)\n",
    "\n",
    "    return input,output,tokenizer,len(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(input,output,input_size,output_size,index=0):\n",
    "    while(True):\n",
    "        for i in range(index,len(input)):\n",
    "            for j in range(15):\n",
    "                encoder_input_list=[]\n",
    "                encoder_mask_list=[]\n",
    "                decoder_input_list=[]\n",
    "                decoder_mask_list=[]\n",
    "                decoder_output_list=[]\n",
    "                batch_size=0\n",
    "                for count in range(5):\n",
    "                    predict_bound=j*20+(count+1)*4\n",
    "                    predict_bound=350 if predict_bound>350 else predict_bound\n",
    "                    knowledge_bound=j*20+(count)*4\n",
    "                    if predict_bound<=len(output[i]):\n",
    "                        batch_size+=1\n",
    "                        temp=padding_mapper(input[i],output[i][:predict_bound],output[i][:knowledge_bound],input_size,output_size)\n",
    "                        ((encoder_input,encoder_mask,decoder_input,decoder_mask),decoder_output)=temp\n",
    "                        encoder_input_list.append(np.array(encoder_input))\n",
    "                        encoder_mask_list.append(np.array(encoder_mask))\n",
    "                        decoder_input_list.append(np.array(decoder_input))\n",
    "                        decoder_mask_list.append(np.array(decoder_mask))\n",
    "                        decoder_output_list.append(np.array(decoder_output))\n",
    "                encoder_input_list=np.array(encoder_input_list)\n",
    "                encoder_mask_list=np.array(encoder_mask_list)\n",
    "                decoder_input_list=np.array(decoder_input_list)\n",
    "                decoder_mask_list=np.array(decoder_mask_list)\n",
    "                decoder_output_list=np.array(decoder_output_list)\n",
    "\n",
    "                if batch_size>0:\n",
    "                    yield ((encoder_input_list,encoder_mask_list,decoder_input_list,decoder_mask_list),decoder_output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/Sagar/.cache/huggingface/datasets/iamketan25___parquet/iamketan25--poem-instructions-dataset-8ab844a537ba1e14/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99e6daf04404700a36f6f1f699fbdbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path=r\"C:\\Users\\Sagar\\Python files_Jupiter\\Git Repo Local\\GAN Poetry\\tokenizers\\tokenizer_{0}.json\"\n",
    "input,output,tokenizer,length = get_ds(\"iamketan25/poem-instructions-dataset\",path,50,300,'train','prompt','chosen')\n",
    "# input,output,tokenizer,length = get_ds(\"checkai/instruction-poems\",path,50,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(input,output,random_state=1,test_size=0.05,shuffle=True)\n",
    "train=data_loader(X_train,Y_train,50,350)\n",
    "test=data_loader(X_test,Y_test,50,350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,training_model,eval_X=None,eval_Y=None,tokenizer=None,input_size=50,output_size=350):\n",
    "        self.training_model=training_model\n",
    "        self.eval_X=eval_X\n",
    "        self.eval_Y=eval_Y\n",
    "        self.tokenizer=tokenizer\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        self.training_model.save(f\"C:/Users/Sagar/Python files_Jupiter/Git Repo Local/GAN Poetry/results/model_{epoch}.h5\")\n",
    "        if (self.eval_X is not None) and (self.eval_Y is not None) and (self.tokenizer is not None):\n",
    "            self.model_evaluate(epoch)\n",
    "\n",
    "    def model_evaluation(self,epoch=0):\n",
    "        count=0\n",
    "        for item in data_loader(self.eval_X,self.eval_Y,self.input_size,self.output_size):\n",
    "            y_predict=self.model.predict(item[0])\n",
    "            y_predict=tf.math.argmax(y_predict,axis=-1)\n",
    "            for i in range(len(y_predict)):\n",
    "                y_hat=self.tokenizer.decode(y_predict[i])\n",
    "                y=self.tokenizer.decode(item[1][i])\n",
    "                with open(f\"C:/Users/Sagar/Python files_Jupiter/Git Repo Local/GAN Poetry/results/logs/transformer_log_epoch_{epoch}_{count}_{i}.txt\",\"w\") as text_file:\n",
    "                    print(f\"y_hat:\\n{y_hat}\\ny:\\n{y}\",file=text_file)\n",
    "            count+=1\n",
    "            if count>=35:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I \\' ve written a poem for you about Living , Death , The Body , Nature . The title is \"[ ready to receive remains . . .]\": ready to receive remains built for death , ready to receive the flat ly desolate super ficial deeply commission ed intellectual offer of suggest ive actions , for the hunger assassin to fall back on and become force full psych ological damage , bottled for drink able agitation . riding a back seat writing construction , contest ing the oncoming molten universe , immersed in villagers , city dwell ers , trembling , laughing , ( white teeth red one for the perfect test of time ), to inhale flesh and stone from long ago , forgetting the horrors of holy oil inf usion clocks and gritty body galleries , leaving behind the mourning river ’ s crimson fragrance smoldering from the previous unbearable fever . in a posture of myself on a speeding body , without hands and feet , I am ready to receive the vomit of consciousness and proceed down the avenues of suggestion to become a limited option .'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
